{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_gpus = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/eng_vocabs.txt\", encoding=\"utf-8\") as f:\n",
    "    dictionary = {}\n",
    "    for i, line in enumerate(f):\n",
    "        token = line.strip(\"\\n\")\n",
    "        dictionary[token] = int(i) + 4\n",
    "        \n",
    "with open(\"../data/jpn_vocabs.txt\", encoding=\"utf-8\") as f:\n",
    "    inv_dictionary = {}\n",
    "    for i, line in enumerate(f):\n",
    "        inv_dictionary[i + 4] = line.strip(\"\\n\")\n",
    "    inv_dictionary[0] = \"<PAD>\"\n",
    "    inv_dictionary[1] = \"<GO>\"\n",
    "    inv_dictionary[2] = \"<UNK>\"\n",
    "    inv_dictionary[3] = \"<EOS>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "emb_size = 512\n",
    "n_hidden = 512\n",
    "en_vocab_size = 131 + 4\n",
    "jp_vocab_size = 3202 + 4\n",
    "n_layers = 4\n",
    "n_attn_heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_embedding(X, Y, reuse=False):\n",
    "    with tf.variable_scope(\"embedding\", reuse=reuse), tf.device(\"/cpu:0\"):\n",
    "        en_emb = tf.get_variable(\"en_emb\", \n",
    "                                [en_vocab_size, emb_size], \n",
    "                                dtype=tf.float32,\n",
    "                                initializer=tf.truncated_normal_initializer(stddev=1e-4))\n",
    "\n",
    "        jp_emb = tf.get_variable(\"jp_emb\", \n",
    "                                [jp_vocab_size, emb_size], \n",
    "                                dtype=tf.float32,\n",
    "                                initializer=tf.truncated_normal_initializer(stddev=1e-4))\n",
    "\n",
    "        P_in = tf.get_variable(\"P_in\", \n",
    "                            [401, emb_size], \n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1e-4))\n",
    "        P_out = tf.get_variable(\"P_out\", \n",
    "                            [402, emb_size], \n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1e-4))\n",
    "\n",
    "        X_emb = tf.nn.embedding_lookup(en_emb, X) + P_in[:tf.shape(X)[1], :]\n",
    "        Y_emb = tf.nn.embedding_lookup(jp_emb, Y) + P_out[:tf.shape(Y)[1], :]\n",
    "        return X_emb, Y_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder_block(inp, n_hidden, filter_size):\n",
    "    inp = tf.expand_dims(inp, 2)\n",
    "    inp = tf.pad(inp, [[0, 0], [(filter_size[0]-1)//2, (filter_size[0]-1)//2], [0, 0], [0, 0]])\n",
    "    conv = slim.convolution(inp, n_hidden, filter_size, data_format=\"NHWC\", padding=\"VALID\", activation_fn=None)\n",
    "    conv = tf.squeeze(conv, 2)\n",
    "    return conv\n",
    "\n",
    "def decoder_block(inp, n_hidden, filter_size):\n",
    "    inp = tf.expand_dims(inp, 2)\n",
    "    inp = tf.pad(inp, [[0, 0], [filter_size[0]-1, 0], [0, 0], [0, 0]])\n",
    "    conv = slim.convolution(inp, n_hidden, filter_size, data_format=\"NHWC\", padding=\"VALID\", activation_fn=None)\n",
    "    conv = tf.squeeze(conv, 2)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def glu(x):\n",
    "    return tf.multiply(x[:, :, :tf.shape(x)[2]//2], tf.sigmoid(x[:, :, tf.shape(x)[2]//2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(inp, conv_block, kernel_width, n_hidden, residual=None):\n",
    "    z = conv_block(inp, n_hidden, (kernel_width, 1))\n",
    "    return glu(z) + (residual if residual is not None else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(inp, n_layers, device):\n",
    "    with tf.variable_scope(\"encoder\", reuse=device > 0), tf.device(\"/gpu:%d\" % device):\n",
    "        e = inp\n",
    "        for i in range(n_layers):\n",
    "            z = layer(inp, encoder_block, 3, n_hidden * 2, inp)\n",
    "            inp = z\n",
    "        return z, z + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder(inp, zu, ze, n_layers, device):\n",
    "    with tf.variable_scope(\"decoder\", reuse=device > 0), tf.device(\"/gpu:%d\" % device):\n",
    "        inp = g = inp\n",
    "        for i in range(n_layers):\n",
    "            attn_res = h = layer(inp, decoder_block, 3, n_hidden * 2, residual=tf.zeros_like(inp))\n",
    "            C = []\n",
    "            for j in range(n_attn_heads):\n",
    "                h_ = slim.linear(h, n_hidden//n_attn_heads)\n",
    "                g_ = slim.linear(g, n_hidden//n_attn_heads)\n",
    "                zu_ = slim.linear(zu, n_hidden//n_attn_heads)\n",
    "                ze_ = slim.linear(ze, n_hidden//n_attn_heads)\n",
    "                \n",
    "                d = slim.linear(h_, n_hidden//n_attn_heads) + g_\n",
    "                dz = tf.matmul(d, tf.transpose(zu_, [0, 2, 1]))\n",
    "                a = tf.nn.softmax(dz)\n",
    "                c_ = tf.matmul(a, ze_)\n",
    "                C.append(c_)\n",
    "            c = tf.concat(C, 2)\n",
    "            h = slim.linear(attn_res + c, n_hidden)\n",
    "#             h = tf.nn.dropout(h, dropout_keep_prob)\n",
    "            inp = h\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(dtype=tf.int32, shape=(None, None))\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=(None, None))\n",
    "n_layers = 4\n",
    "en_emb, jp_emb = init_embedding(X, Y, reuse=False)\n",
    "zu, ze = encoder(en_emb, n_layers, device=0)\n",
    "\n",
    "zu_placeholder = tf.placeholder(zu.dtype, zu.get_shape())\n",
    "ze_placeholder = tf.placeholder(ze.dtype, ze.get_shape())\n",
    "hg = decoder(jp_emb, zu_placeholder, ze_placeholder, n_layers, device=0)\n",
    "\n",
    "with tf.variable_scope(\"logits\"):\n",
    "    logits = slim.fully_connected(hg, jp_vocab_size)\n",
    "    logits = logits[:, :-1]\n",
    "    p = tf.nn.softmax(logits)\n",
    "    k = tf.placeholder(dtype=tf.int32)\n",
    "    topk_logprobs, topk_ids = tf.nn.top_k(tf.log(p), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def encode(sess, enc_inputs):\n",
    "    input_feed = {X: enc_inputs}\n",
    "    outputs = [zu, ze]\n",
    "    return sess.run(outputs, input_feed)\n",
    "\n",
    "def decode_topk(sess, zu_inputs, ze_inputs, dec_inputs, beam_size):\n",
    "    input_feed = {zu_placeholder: zu_inputs, \n",
    "                  ze_placeholder: ze_inputs,\n",
    "                  k: beam_size * 2,\n",
    "                  Y: dec_inputs}\n",
    "    outputs = [topk_logprobs, topk_ids]\n",
    "    return sess.run(outputs, input_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Hypothesis(object):\n",
    "    def __init__(self, log_prob, seq):\n",
    "        self.log_prob = log_prob\n",
    "        self.seq = seq\n",
    "    \n",
    "    @property\n",
    "    def step(self):\n",
    "        return len(self.seq) - 1\n",
    "\n",
    "\n",
    "def beam_search(sess, zu_inputs, ze_inputs, batch_size, beam_size, num_ans, max_len, normalize_by_len=1.):        \n",
    "    assert 0 <= normalize_by_len <= 1\n",
    "    # decode\n",
    "    dec_inputs = np.ones((batch_size, 2), dtype=np.int32)\n",
    "    answers = [[] for i in range(batch_size)]\n",
    "    H = [[] for i in range(batch_size)]\n",
    "    \n",
    "    tkl, tkid = decode_topk(sess, zu_inputs, ze_inputs, dec_inputs, beam_size)  # step, batch_size, k\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j, log_prob in enumerate(tkl[i, 0]):  # only need to consider step 0th\n",
    "            if tkid[i, 0, j] != 3:\n",
    "                h = Hypothesis(log_prob, [1, tkid[i, 0, j]])\n",
    "                H[i].append(h)\n",
    "        H[i].sort(key=lambda h: h.log_prob)\n",
    "\n",
    "    done = [False] * batch_size\n",
    "    while not all(done):            \n",
    "        tkl_beam = []\n",
    "        tkid_beam = []\n",
    "        dec_inputs_beam = []\n",
    "        steps_beam = []\n",
    "        for i in range(beam_size):\n",
    "            steps = [1] * batch_size\n",
    "            prev_log_probs = np.zeros(batch_size, dtype=np.float32)\n",
    "            dec_inputs = np.ones((batch_size, max_len), dtype=np.int32)\n",
    "            for j, h in enumerate(H):\n",
    "                while h:\n",
    "                    hi = h.pop()\n",
    "                    lp, step, candidate_seq = hi.log_prob, hi.step, hi.seq\n",
    "                    if candidate_seq[-1] != 3:\n",
    "                        dec_inputs[j, :len(candidate_seq)] = candidate_seq\n",
    "                        steps[j] = step\n",
    "                        prev_log_probs[j] = lp\n",
    "                        break\n",
    "                    else:\n",
    "                        answers[j].append((lp, candidate_seq))\n",
    "            max_step = max(steps)\n",
    "            dec_inputs = dec_inputs[:, :max_step + 2]\n",
    "            tkl, tkid = decode_topk(sess, zu_inputs, ze_inputs, dec_inputs, beam_size)  # step, batch_size, k\n",
    "            tkl_beam.append(tkl + prev_log_probs[:, None, None])\n",
    "            tkid_beam.append(tkid)\n",
    "            dec_inputs_beam.append(dec_inputs.copy())\n",
    "            steps_beam.append(steps)\n",
    "            \n",
    "        for i in range(beam_size):\n",
    "            tkl = tkl_beam[i]\n",
    "            tkid = tkid_beam[i]\n",
    "            dec_inputs = dec_inputs_beam[i]\n",
    "            steps = steps_beam[i]\n",
    "            for j in range(batch_size):\n",
    "                step = steps[j]\n",
    "                for k in range(tkid.shape[2]):\n",
    "                    extended_seq = np.hstack((dec_inputs[j, :step+1], [tkid[j, step, k]]))\n",
    "                    log_prob = tkl[j, step, k]\n",
    "                    if len(extended_seq) <= max_len and log_prob > -10:\n",
    "                        h = Hypothesis(log_prob, extended_seq)\n",
    "                        H[j].append(h)\n",
    "\n",
    "                H[j].sort(key=lambda h: h.log_prob / (h.step**normalize_by_len))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            done[i] = (len(answers[i]) >= num_ans) or (not H[i]) or (len(H[i]) > 1000)\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def format_log(ans):\n",
    "    out = \"\"\n",
    "    for log_prob, kw in ans:\n",
    "        out += \"\\t[%.4f] %s\\n\" % (log_prob, to_text(kw, inv_dictionary))\n",
    "    return out\n",
    "\n",
    "def to_text(data, inv_dictionary, ignore_special_char=True, reverse=False):\n",
    "    if ignore_special_char:\n",
    "        data = [x for x in data if x > 3]\n",
    "    if reverse:\n",
    "        return \"\".join([inv_dictionary[i] for i in data[::-1]])\n",
    "    return \"\".join([inv_dictionary[i] for i in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "isess = tf.InteractiveSession(config=config)\n",
    "isess.run(tf.global_variables_initializer())\n",
    "saver.restore(isess, \"../checkpoints/tatoeba_v2/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "    max_len = min(400, 1 + np.max([len(t) for t in text]))\n",
    "    T = np.zeros((len(text), 400), dtype=np.int32)\n",
    "    for i, t in enumerate(text):\n",
    "        for j, x in enumerate(t):\n",
    "            T[i, j] = dictionary.get(x, 2)\n",
    "        T[i, j + 1] = 3\n",
    "    zu_inputs, ze_inputs = encode(isess, T)\n",
    "    answers = beam_search(isess, zu_inputs, ze_inputs, batch_size=T.shape[0], beam_size=4, num_ans=10, max_len=402, normalize_by_len=0.1)\n",
    "    for i, ans in enumerate(answers):\n",
    "        ans.sort(key=lambda x: x[0], reverse=True)\n",
    "        print(format_log(ans))\n",
    "        print(len(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[-1.6895] 明日だ。\n",
      "\t[-2.5908] 明日。\n",
      "\t[-3.1339] 明日だよ。\n",
      "\t[-3.5270] 明日です。\n",
      "\t[-4.6687] 風が強い。\n",
      "\t[-4.8581] 明日なんだ。\n",
      "\t[-5.1890] 明日だ！\n",
      "\t[-5.2565] 明日ですよ。\n",
      "\t[-5.4912] 明日なんです。\n",
      "\t[-5.6846] 明日は風が強い。\n",
      "\t[-5.7157] 風が強いです。\n",
      "\t[-5.8479] 明日なんだよ。\n",
      "\t[-5.8654] 明日、風がない。\n",
      "\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "translate([\"It's windy tomorrow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Visualize Attentions\n",
    "def compute_attentions(isess, X_in, Y_in):\n",
    "    zu_in, ze_in = encode(isess, X_in)\n",
    "    input_feed = {zu_placeholder: zu_in, \n",
    "                  ze_placeholder: ze_in,\n",
    "                  Y: Y_in}\n",
    "    return isess.run(A, input_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "titles = [\"It's raining tomorrow.\"]\n",
    "kws = [\"明日は雨だ。\"]\n",
    "\n",
    "max_len = min(201, 1 + np.max([len(t) for t in titles]))\n",
    "X_in = np.zeros((len(titles), max_len), dtype=np.int32)\n",
    "\n",
    "max_len = min(22, 2 + np.max([len(t) for t in kws]))\n",
    "Y_in = np.zeros((len(kws), max_len), dtype=np.int32)\n",
    "\n",
    "for i, t in enumerate(titles):\n",
    "    for j, x in enumerate(t):\n",
    "        X_in[i, j] = dictionary.get(x, 2)\n",
    "    X_in[i, j + 1] = 3\n",
    "\n",
    "for i, t in enumerate(kws):\n",
    "    for j, x in enumerate(t):\n",
    "        Y_in[i, j + 1] = dictionary.get(x, 2)\n",
    "    Y_in[i, 0] = 1\n",
    "    Y_in[i, j+2] = 3\n",
    "\n",
    "\n",
    "attns = compute_attentions(isess, X_in, Y_in)\n",
    "\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib import rcParams\n",
    "\n",
    "fp = FontProperties(fname='/home/jo/.fonts/fonts-horai-umefont-670/ume-hgo4.ttf', size=16)\n",
    "rcParams['font.family'] = fp.get_name()\n",
    "\n",
    "plt.gcf().set_size_inches(40, 40)\n",
    "for i, a in enumerate(attns):\n",
    "    plt.subplot(len(attns), 1, i + 1)\n",
    "#     plt.title(\"attn %d\" % i)\n",
    "    plt.xticks(range(a[0].shape[1]), [en_inv_dictionary[i] for i in X_in[0]], size=16)\n",
    "    plt.yticks(range(a[0].shape[0]), [jp_inv_dictionary[i] for i in Y_in[0][1:]], size=16)\n",
    "    plt.imshow(a[0], interpolation=\"nearest\", cmap=plt.cm.Greens) \n",
    "    plt.gca().grid(True, linestyle='--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
